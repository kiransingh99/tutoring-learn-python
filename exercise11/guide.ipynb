{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Guide: Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook we introduced algorithms as structured recipes for performing tasks. There are often different algorithms for performing the same task. We have seen this for root finding (bisection and Newton's method) and for sorting. We now want to analyse the performance of some algorithms, and in particular understand how the required time (or maybe the required memory) changes as the problem size increases. This is known as *algorithmic complexity*, and it helps us to pick an appropriate algorithm for a given problem, and to determine whether or not an algorithm will be able to solve a problem of a given size within an acceptable time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Understand 'big-oh' ($O$) notation\n",
    "- Appreciate the consequences of algorithmic complexity\n",
    "- Determine the complexity of some simple algorithms\n",
    "- Measure complexity experimentally\n",
    "\n",
    "> Students frequently find complexity a challenging topic. Take your time to work through examples and think about how calculations you would need to perform if computing a problem by hand.\n",
    "\n",
    "**NOTE:** This notebook can take some time to execute as it performs timings for a selection of algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, we import NumPy and Matplotlib, and configure Matplotlib for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity and 'big Oh' notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a problem of size $n$ (if we were sorting an array of numbers, $n$ would be the length of the array). For many algorithms, when $n$ is large we can express the time cost $t$ as:\n",
    "\n",
    "$$ t = C g(n) $$\n",
    "\n",
    "where $C$ is a constant and $g$ is a function. If the cost can be expressed as above, where $C$ is a constant, then we write in 'big-Oh' notation:\n",
    "\n",
    "$$ t = O(g(n)) $$\n",
    "\n",
    "We consider some common expressions for $g(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an algorithm that is *constant in time*, we have $t = O(1)$. This means that the time required for the algorithm is *independent* of the problem size $n$. An example of an $O(1)$ algorithm is accessing a specified entry in an array by index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an algorithm that is *polynomial in time*, we have\n",
    "\n",
    "$$ t = O(n^k) $$\n",
    "\n",
    "where $k \\ge 1$ is a constant (it does not have to be an integer). Common cases are:\n",
    "\n",
    "- $O(n)$: linear complexity\n",
    "- $O(n^2)$: quadratic complexity\n",
    "- $O(n^3)$: cubic complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logarithmic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an algorithm that is *logarithmic in time*, we have $t = O(\\log n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loglinear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of important algorithms are *log-linear in time*, that is $t = O(n\\log n)$. Perhaps the most famous $O(n\\log n)$ algorithm is the fast Fourier transform (FFT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some algorithms are *exponential in time*, that is $t = O(c^{n})$, where $c \\ge 1$. Clearly such algorithms become extremely expensive for large $n$; they are generally of little or no practical use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To develop a sense of different complexities, below we compute a table of the $g(n)$ terms for common complexities (study the code if you wish, but it's not required). Remember that the required time is proportional to $g(n)$, so we are interested in the relative change as we increase $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "N = (10, 100, 1000, 10000, 100000, 1000000)\n",
    "functions = ((\"1\", lambda n: 1),\n",
    "             (\"n\", lambda n: n/N[0]),\n",
    "             (\"n**2\", lambda n: n**2/N[0]**2),\n",
    "             (\"n**3\", lambda n: n**3/N[0]**3),\n",
    "             (\"log(n)\", lambda n: math.log(n)/math.log(N[0])),\n",
    "             (\"nlog(n)\", lambda n: n*math.log(n)/(N[0]*math.log(N[0]))))\n",
    "\n",
    "\n",
    "# Create table header\n",
    "header = \"{:<9}| \".format(\"n\")\n",
    "for f in functions:\n",
    "    header += \"{:<16}\".format(f[0])\n",
    "print(header)\n",
    "\n",
    "# Print divider\n",
    "print(\"-\"*len(header))\n",
    "\n",
    "# Print rows\n",
    "for n in N:\n",
    "    row = \"{:<9}| \".format(n)\n",
    "    for f in functions:\n",
    "        row += \"{:<16.4e}\".format(f[1](n))\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $g(n)$ terms have been normalised using $g(10)$ so we can compare more easily the different cases. The table highlights the dramatic increase in cost for the $n^{2}$ and $n^{3}$ cases as $n$ becomes larger. Algorithms with lower complexity, e.g. $\\log(n)$, $n$ and $n\\log(n)$, are much more appealing.\n",
    "\n",
    "We have discussed complexity in terms of time - how long an algorithm would take to execute - but we could also have discussed complexity in terms of *space* (memory). It can be important to know how the computer memory required by an algorithm will change with problem size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the complexity of an algorithm, we just need to count the number of operations. If we have an array `x` of length $n$, and multiply it by a scalar $a$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100000\n",
    "x = np.random.rand(n)\n",
    "\n",
    "a = 10.0\n",
    "for i in range(n):\n",
    "    x[i] = a*x[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost of the operation ```x[i] = a*x[i]``` is $O(1)$ for each ```i```, and it is performed $n$ times, so overall the cost is $O(n)$.\n",
    "\n",
    "If we have a $m \\times n$ matrix and multiply it by the scalar $a$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 500\n",
    "n = 1000\n",
    "A = np.random.rand(m, n)\n",
    "\n",
    "a = 10.0\n",
    "for i in range(m):\n",
    "    for j in range(n):\n",
    "        A[i, j] = a*A[i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each ```i```, the loop over ```j``` is executed $n$ times with a total cost of $O(n)$ for each ```i```. We loop over ```i``` $m$ times with a cost of $O(n)$ for each loop. Hence the total cost is $O(mn)$. For a square matrix ($m = n$) the complexity is $O(n^{2})$.\n",
    "\n",
    "Typical operations on $n \\times n$ matrices have complexity that is higher than $O(n)$, so operations on large matrices can be very expensive and algorithms need to be chosen carefully when $n$ is large.\n",
    "\n",
    "The above examples are straightforward as they do not depend on the data that is stored in the vector or matrix - the number of operations depends *only* on the problem size $n$. For other algorithms, such as search and sorting, the complexity can depend on the initial data, for example whether or not an array is sorted. In these cases,  when assessing an algorithm we consider:\n",
    "\n",
    "- Best case complexity\n",
    "- Worst case complexity\n",
    "- Average case complexity\n",
    "\n",
    "When an algorithm is presented, the above complexities are often given together with the conditions under which each case is met. We will see practical examples of best and worst case complexities below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost of operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that the cost of operations such as addition, subtraction, multiplication and division is $O(1)$, i.e. a constant. You might find in some algorithm books that the cost of an operation depends on the number of digits in a number. This is only relevant for extremely large numbers, beyond what we encounter in normal scientific and engineering applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook we looked at linear and binary search. We now want to consider the complexity and test it experimentally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With linear search we iterate over an array looking for a particular value. If we are lucky, the value we are searching for will be in the first few places of the array, in which case the complexity will be $O(1)$ - it does not depend on the array length. If the value is not in the array, we have to check every entry to verify this. The array has $n$ entries, so this has complexity $O(n)$. If the array does contain the value we are looking for, maybe on average it will be in the middle, so we have to perform $n/2$ checks on average. Since we are not interested in the constant term ($1/2$), the complexity for this case (average case) is $O(n)$.  This leads us to the following complexities for linear search:\n",
    "\n",
    "- Best case: $O(1)$\n",
    "- Worst case: $O(n)$\n",
    "- Average case: $O(n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary search (covered in the previous notebook) has complexity:\n",
    "\n",
    "- Best case: $O(1)$\n",
    "- Worst case: $O(\\log n)$\n",
    "- Average case: $O(\\log n)$\n",
    "\n",
    "For large $n$, binary search is *much* faster than linear search.\n",
    "\n",
    "Despite the differences in complexity, for small problems linear search is faster than binary search. This is because the 'proportionality' constant ($C$) is smaller for linear search. A reason is that modern processors are optimised for moving over data in arrays in sequence - they exploit the processor *cache*, which is small but fast memory that is located on the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing search performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the complexities for linear and binary search, we can test the performance experimentally using the implementations from the previous notebook. We start with linear search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_search(x, val):\n",
    "    \"Return True if val is in x, otherwise return False\"\n",
    "    for item in x:\n",
    "        if item == val:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test the complexity by searching for a value in arrays of increasing length, and timing how long it takes. We will use the magic function [`%timeit`](./Notebook%20tips.ipynb#Detailed-timing) to get the execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array of problem sizes n we want to test (powers of 2)\n",
    "N = 2**np.arange(2, 20)\n",
    "\n",
    "# Generate the array of integers for the largest problem to use in plotting times\n",
    "x = np.arange(N[-1])\n",
    "\n",
    "# Initialise an empty array to stores times for plotting\n",
    "times = []\n",
    "\n",
    "# Time the search for each problem size\n",
    "for n in N:\n",
    "\n",
    "    # Time search function (repeating 3 times) to find a random integer in x[:n]\n",
    "    t = %timeit -q -n4 -r1 -o linear_search(x[:n], np.random.randint(0, n))\n",
    "\n",
    "    # Store best case time\n",
    "    times.append(t.best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the time as a function of problem size, on a log-log scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and label the time taken for linear search\n",
    "plt.loglog(N, times, marker='o', label='linear search')\n",
    "plt.xlabel('$n$')\n",
    "plt.ylabel('$t$ (s)')\n",
    "\n",
    "# Show a reference line of O(n)\n",
    "plt.loglog(N, 1e-6*N, label='$O(n)$')\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we see that as $n$ increases the time required is proportional to $n$.\n",
    "\n",
    "We repeat this experiment, but this time for binary search. Here is the search function from the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search(x, val):\n",
    "    \"Peform binary search on x to find val. If found returns position, otherwise returns None.\"\n",
    "\n",
    "    # Intialise end point indices\n",
    "    lower, upper = 0, len(x) - 1\n",
    "\n",
    "    # If values is outside of interval, return None\n",
    "    if val < x[lower] or val > x[upper]:\n",
    "        return None\n",
    "\n",
    "    # Perform binary search\n",
    "    while True:\n",
    "\n",
    "        # Compute midpoint index (integer division)\n",
    "        midpoint = (upper + lower) // 2\n",
    "\n",
    "        # Check which side of x[midpoint] val lies, and update midpoint accordingly\n",
    "        if val < x[midpoint]:\n",
    "            upper = midpoint - 1\n",
    "        elif val > x[midpoint]:\n",
    "            lower = midpoint + 1\n",
    "        elif val == x[midpoint]:  # found, so return\n",
    "            return midpoint\n",
    "\n",
    "        # In this case val is not in list (return None)\n",
    "        if upper < lower:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary sort is much faster than linear search, so we need to test it for larger arrays than we did for linear search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array of problem sizes we want to test (powers of 2)\n",
    "N = 2**np.arange(2, 24)\n",
    "\n",
    "# Creat array and sort\n",
    "x = np.arange(N[-1])\n",
    "x = np.sort(x)\n",
    "\n",
    "# Initlise an empty array to capture time taken\n",
    "times = []\n",
    "\n",
    "# Time search for different problem sizes\n",
    "for n in N:\n",
    "    # Time search function for finding '2'\n",
    "    t = %timeit -q -n5 -r2 -o binary_search(x[:n], 2)\n",
    "\n",
    "    # Store average\n",
    "    times.append(t.best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect $O(\\log n)$ complexity, so we will use a log scale for $n$ and a linear scale for time $t$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and label the time taken for binary search\n",
    "plt.semilogx(N, times, marker='o', label='binary search')\n",
    "plt.xlabel('$n$')\n",
    "plt.ylabel('$t$ (s)')\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc=0)\n",
    "\n",
    "# Change format on y-axis to scientific notation\n",
    "plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a line that is close to linear on the semi-log plot, i.e. binary search is $O(\\log(n))$. Binary search is so fast that it can be hard to get good timings for small problems because noise, caused by things like other processes running on a computer, can be significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to sort a random sequence into ascending order. This is a very common and extensively studied problem. There are over 40 sort algorithms listed on [Wikipedia](https://en.wikipedia.org/wiki/Sorting_algorithm). Which one do we pick? Let's test bubble sort and quicksort to see how they perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bubble sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduced bubble sort in the previous notebook. For an array of length $n$, it involves iterating over all entries and performing swaps. This has cost $O(n)$. We then repeat this $n$ times. Hence, bubble sort has complexity $O(n^{2})$.\n",
    "\n",
    "Below is the bubble sort implementation from the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bubble_sort(A):\n",
    "    \"Sort A and return\"\n",
    "    A = A.copy()\n",
    "    n = len(A)\n",
    "    while n > 0:\n",
    "        for i in range(n - 1):\n",
    "            # Swap data if in wrong order\n",
    "            if A[i] > A[i + 1]:\n",
    "                A[i + 1], A[i] = A[i], A[i + 1]\n",
    "        n = n - 1\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can determine the complexity of bubble sort by inspecting the code. It involves a *nest* of two loops (a ```while``` and a ```for``` loop), and each is executed $n-1$ times. Swapping the data is $O(1)$ (no dependency on $n$), hence the complexity for the whole algorithm is $O(n^{2})$.\n",
    "\n",
    "Let's time the algorithm for increasingly large arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array of problem sizes we want to test (powers of 2)\n",
    "N = 2**np.arange(2, 10)\n",
    "\n",
    "# Create an array of random numbers\n",
    "x = np.random.rand(N[-1])\n",
    "\n",
    "# Time bubble sort on arrays of different lengths\n",
    "times = []\n",
    "for n in N:\n",
    "    t = %timeit -q -n2 -r2 -o bubble_sort(x[:n])\n",
    "    times.append(t.best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the bubble sort time against the size of the array on a log-log plot, and compare against a $O(n^{2})$ line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bubble sort timing\n",
    "plt.loglog(N, times, marker='o', label='bubble sort')\n",
    "\n",
    "# Show reference line of O(n^2)\n",
    "plt.loglog(N, 1e-6*N**2, label='$O(n^2)$')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('$n$')\n",
    "plt.ylabel('$t$ (s)')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that for bubble sort that $t \\propto n^2$ -  we are observing the $O(n^2)$ complexity. This makes bubble sort too expensive for large $n$ to be of practical use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quicksort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysing the complexity of quicksort is not as simple at the preceding algorithms, so we will state its complexity and test it experimentally. Quicksort has complexities:\n",
    "\n",
    "- Best case: $O(n\\log n)$\n",
    "- Worst case: $O(n^{2})$\n",
    "- Average case: $O(n\\log n)$\n",
    "\n",
    "The worst case complexity occurs when the data is already sorted.\n",
    "\n",
    "We reproduce here the quicksort algorithm from the previous notebook so we can test its complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quicksort(A, lo=0, hi=None):\n",
    "    \"Sort A and return sorted array\"\n",
    "\n",
    "    # Initialise data the first time function is called\n",
    "    if hi is None:\n",
    "        hi = len(A) - 1\n",
    "        A = A.copy()\n",
    "\n",
    "    # Sort\n",
    "    if lo < hi:\n",
    "        p = partition(A, lo,  hi)\n",
    "        quicksort(A, lo, p - 1)\n",
    "        quicksort(A, p + 1, hi)\n",
    "    return A\n",
    "\n",
    "\n",
    "def partition(A, lo, hi):\n",
    "    \"Partitioning function for use in quicksort\"\n",
    "    pivot = A[hi]\n",
    "    i = lo\n",
    "    for j in range(lo,  hi):\n",
    "        if A[j] <= pivot:\n",
    "            A[i], A[j] = A[j], A[i]\n",
    "            i += 1\n",
    "    A[i], A[hi] = A[hi], A[i]\n",
    "    return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gather timings for quicksort to sort an array of random numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array of problem sizes we want to test (powers of 2)\n",
    "N = 2**np.arange(2, 14)\n",
    "\n",
    "# Create an array of random numbers\n",
    "x = np.random.rand(N[-1])\n",
    "\n",
    "# Time quicksort on arrays of different lengths\n",
    "times = []\n",
    "for n in N:\n",
    "    t = %timeit -n1 -r1 -o -q quicksort(x[:n])\n",
    "    times.append(t.best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the results, with a $O(n\\log(n))$ line as a reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot quicksort timings\n",
    "plt.loglog(N, times, marker='o', label='quicksort')\n",
    "\n",
    "# Show reference line of O(n*log(n))\n",
    "plt.loglog(N, 1e-6*N*np.log(N), label='$O(n\\log\\, n)$')\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel('$n$')\n",
    "plt.ylabel('$t$ (s)')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe here the $O(n\\log n)$ complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Worst case complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reproduce the quicksort worst-case complexity, which is $O(n^{2})$, by pre-sorting an array. Since an $O(n^{2})$ algorithm will be much more expensive than $O(n\\log(n))$ for large $n$, we will test for smaller problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array of problem sizes we want to test (powers of 2)\n",
    "N = 2**np.arange(2, 12)\n",
    "\n",
    "# Create array of random numbers and pre-sort\n",
    "x = np.random.rand(N[-1])\n",
    "x = np.sort(x)\n",
    "\n",
    "# Quicksort is recursive, but by deafult Python raises an exeption if the number\n",
    "# of recursions is high (to avoid crashes due to accidental infinite recursion loops),\n",
    "# so we need to increase the limit\n",
    "import sys\n",
    "sys.setrecursionlimit(25000)\n",
    "\n",
    "times = []\n",
    "for n in N:\n",
    "    t = %timeit -n1 -r1 -o -q quicksort(x[:n])\n",
    "    times.append(t.best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the timing results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot quicksort timings\n",
    "plt.loglog(N, times, marker='o', label='quicksort (ordered data)')\n",
    "\n",
    "# Plot nlog(n) line for reference\n",
    "plt.loglog(N, 1e-6*N*np.log(N), '--', label=r'$O(n\\log n)$')\n",
    "\n",
    "# Show reference line of O(n^2)\n",
    "plt.loglog(N, 1e-6*N**2, '--', label=r'$O(n^2$)')\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel('$n$')\n",
    "plt.ylabel('$t$ (s)')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we observe $O(n^{2})$ complexity for the pre-sorted case - we have reproduced the worst-case scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library sort implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to see how library (built-in) sorting implementations compare to our implementations. The built-in Python function ```sorted``` is an implementation of Timsort (https://en.wikipedia.org/wiki/Timsort). We compare the performance of ```sorted``` and ```numpy.sort``` (using quicksort) to our quicksort implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array of problem sizes we want to test (powers of 2)\n",
    "N = 2**np.arange(2, 14)\n",
    "\n",
    "# Create an array of random numbers, and make read-only so we don't accidentally sort it\n",
    "x = np.random.rand(N[-1])\n",
    "x.flags.writeable = False\n",
    "\n",
    "# Time the different implementations\n",
    "our_times = []\n",
    "py_times = []\n",
    "np_times = []\n",
    "for n in N:\n",
    "    # Time our quicksort implememtation\n",
    "    t = %timeit -n3 -q -o quicksort(x[:n])\n",
    "    our_times.append(t.best)\n",
    "\n",
    "    # Time Python built-in sort\n",
    "    t = %timeit -n3 -q -o sorted(x[:n])\n",
    "    py_times.append(t.best)\n",
    "\n",
    "    t = %timeit -n3 -q -o np.sort(x[:n], kind='quicksort')\n",
    "    np_times.append(t.best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time taken for quicksort of our implementation\n",
    "plt.loglog(N, our_times, marker='o', label='Our quicksort')\n",
    "\n",
    "# Plot time taken for built-in sort\n",
    "plt.loglog(N, py_times, marker='o', label='Python (timsort)')\n",
    "plt.loglog(N, np_times, marker='o', label='NumPy (quicksort)')\n",
    "\n",
    "# Show reference lines of O(n*log(n)) and  O(n^2)\n",
    "plt.loglog(N, 1e-6*N*np.log(N), '--', label=r'$O(n\\log n)$')\n",
    "plt.loglog(N, 1e-6*N**2, '--', label=r'$O(n^2$)')\n",
    "\n",
    "# Show legend\n",
    "plt.legend(loc=0)\n",
    "\n",
    "# Add label and legend\n",
    "plt.xlabel('$n$')\n",
    "plt.ylabel('$t$ (s)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all three implementations have $O(n\\log(n))$ complexity, but there are large differences in speed (keep in mind the log scale). Our implementation is by far the slowest!\n",
    "\n",
    "Repeating this test on sorted data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array of problem sizes we want to test (powers of 2)\n",
    "N = 2**np.arange(2, 12)\n",
    "\n",
    "# Create an array of random numbers and sort\n",
    "x = np.random.rand(N[-1])\n",
    "x.sort()\n",
    "\n",
    "# Time the different implementations\n",
    "our_times = []\n",
    "py_times = []\n",
    "np_times = []\n",
    "for n in N:\n",
    "    # Time our quicksort implememtation\n",
    "    t = %timeit -n3 -q -o quicksort(x[:n])\n",
    "    our_times.append(t.best)\n",
    "\n",
    "    # Time Python built-in sort\n",
    "    t = %timeit -n3 -q -o sorted(x[:n])\n",
    "    py_times.append(t.best)\n",
    "\n",
    "    t = %timeit -n3 -q -o np.sort(x[:n], kind='quicksort')\n",
    "    np_times.append(t.best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time taken for quicksort of our implementation\n",
    "plt.loglog(N, our_times, marker='o', label='Our quicksort sort')\n",
    "\n",
    "# Plot time taken for built-in quicksort\n",
    "plt.loglog(N, py_times, marker='o', label='Python sort (timsort)')\n",
    "plt.loglog(N, np_times, marker='o', label='numpy sort (quicksort)')\n",
    "\n",
    "# Show reference line of O(n*log(n)) and O(n^2)\n",
    "plt.loglog(N, 1.0e-7*N*np.log(N), '--', label=r'$O(n \\log n)$')\n",
    "plt.loglog(N, 1.0e-6*N**2, '--',  label=r'$O(n^2)$')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('$n$')\n",
    "plt.ylabel('$t$ (s)')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our implementation exhibits the worst case $O(n^2)$ complexity, while the NumPy quicksort implementation is $O(n\\log(n))$ despite the input list being sorted. Quality implementations have heuristics to avoid the worst-case complexity. Timsort has worst-case complexity of $O(n\\log n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complexity is important as it tells how the cost of an algorithm will increase as problem size increases and in practice whether or not an algorithm will be suitable for an application. For large problems it is important to select algorithms, or possible develop new algorithms, with low complexity.\n",
    "\n",
    "We have focused on time complexity, but remember there is also *space complexity*, which is how the required memory changes with problem size. If you have a device with limited memory, you might favour an algorithm with low space complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now have a go at the [exercises](exercises.ipynb) notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
